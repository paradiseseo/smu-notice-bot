# main.py
import os, re, json, time, hashlib
from urllib.parse import urljoin, urlparse, parse_qs
import requests
from bs4 import BeautifulSoup

BASE = "https://www.smu.ac.kr"
LIST_URL = "https://www.smu.ac.kr/kor/life/notice.do"
# ëª©ë¡ì„ ì•ˆì •ì ìœ¼ë¡œ ë³´ì—¬ì£¼ëŠ” ì¿¼ë¦¬ ì…‹(ìº í¼ìŠ¤/ê¸°ê°„/ìƒë‹¨ê³µì§€ í¬í•¨)
LIST_URL_WITH_PARAMS = (
    LIST_URL
    + "?srCampus=smu&srUpperNoticeYn=on"
    + "&srStartDt=2024-03-01&srEndDt=2026-02-28"
    + "&article.offset=0&articleLimit=10"
)
USER_AGENT = "Mozilla/5.0 (compatible; smu-notice-bot/1.0; +https://www.smu.ac.kr)"
TIMEOUT = 20

STATE_PATH = "state.json"
MAX_SEND_PER_RUN = 10      # 1íšŒ ì‹¤í–‰ ì‹œ ìµœëŒ€ ì „ì†¡ ê°œìˆ˜ (ìŠ¤íŒ¸ ë°©ì§€)
KEYWORDS = [               # (ì„ íƒ) í•„í„°ë§ í‚¤ì›Œë“œ â€” ì´ˆê¸°ì—ëŠ” ëª¨ë‘ ì „ì†¡í•˜ë ¤ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ë‘ì„¸ìš”.
    # r"ì¥í•™", r"ë“±ë¡", r"ìˆ˜ê°•", r"ì±„ìš©", r"ëª¨ì§‘", r"ê³µëª¨ì „", r"ëŒ€íšŒ", r"í–‰ì‚¬"
]

WEBHOOK_URL = os.environ.get("DISCORD_WEBHOOK_URL")
if not WEBHOOK_URL:
    raise SystemExit("í™˜ê²½ë³€ìˆ˜ DISCORD_WEBHOOK_URL ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

def send_test():
    import requests
    requests.post(WEBHOOK_URL, json={"content":"ğŸ§ª FORCE_SEND í…ŒìŠ¤íŠ¸"}, timeout=TIMEOUT)

if os.environ.get("FORCE_SEND") == "1":
    print("[DEBUG] FORCE_SEND=1 â†’ í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€ ì „ì†¡")
    send_test()

def load_seen():
    try:
        with open(STATE_PATH, "r", encoding="utf-8") as f:
            data = json.load(f)
            if isinstance(data, list):
                return set(data)
            return set(data.get("seen", []))
    except FileNotFoundError:
        return set()

def save_seen(seen_set):
    with open(STATE_PATH, "w", encoding="utf-8") as f:
        json.dump(sorted(seen_set), f, ensure_ascii=False, indent=2)

def http_get(url, **kwargs):
    headers = kwargs.pop("headers", {})
    headers["User-Agent"] = USER_AGENT
    return requests.get(url, headers=headers, timeout=TIMEOUT, **kwargs)

def extract_id_from_url(href: str) -> str:
    """
    URL ì¿¼ë¦¬ì˜ no, bbsNo ê°™ì€ íŒŒë¼ë¯¸í„°ê°€ ìˆìœ¼ë©´ ê·¸ê±¸ ì“°ê³ ,
    ì—†ìœ¼ë©´ hrefë¥¼ í•´ì‹œë¡œ IDí™”.
    """
    try:
        qs = parse_qs(urlparse(href).query)
        for k in ("no", "bbsNo", "articleNo", "nttNo"):
            if k in qs and qs[k]:
                return f"{k}:{qs[k][0]}"
    except Exception:
        pass
    return "hash:" + hashlib.sha1(href.encode("utf-8")).hexdigest()[:16]

def clean_title(raw: str) -> str:
    """'í†µí•©ê³µì§€ ê²Œì‹œíŒì½ê¸°(...)' ê°™ì€ ê³µí†µ í”„ë¦¬í”½ìŠ¤ ì œê±° + ê´„í˜¸ ë‚´ìš©ë§Œ ì¶”ì¶œ"""
    t = (raw or "").strip()
    # í†µí•©ê³µì§€ ê²Œì‹œíŒì½ê¸° / ê²Œì‹œíŒ ì½ê¸° / ê³µë°± ë³€í˜• ì œê±°
    t = re.sub(r'^\s*í†µí•©ê³µì§€\s*ê²Œì‹œíŒ\s*ì½ê¸°\s*', '', t)
    t = re.sub(r'^\s*í†µí•©ê³µì§€\s*ê²Œì‹œíŒì½ê¸°\s*', '', t)
    t = re.sub(r'^\s*\[\s*í†µí•©ê³µì§€\s*\]\s*', '', t)
    # ì „ì²´ê°€ ê´„í˜¸ë¡œ ë‘˜ëŸ¬ì‹¸ì¸ í˜•íƒœë©´ ì•ˆìª½ë§Œ
    m = re.match(r'^\((.+)\)$', t)
    if m:
        t = m.group(1).strip()
    # ê³µë°± ì •ë¦¬
    t = re.sub(r'\s+', ' ', t).strip()
    return t

def match_keywords(title: str) -> bool:
    if not KEYWORDS:
        return True
    return any(re.search(p, title, flags=re.I) for p in KEYWORDS)

def fetch_list_items():
    """
    ëª©ë¡ HTMLì—ì„œ articleNoë¥¼ ì •ê·œì‹ìœ¼ë¡œ ìˆ˜ì§‘í•˜ê³ ,
    ê° ìƒì„¸ í˜ì´ì§€(/kor/life/notice.do?mode=view&articleNo=...)ë¡œ ë“¤ì–´ê°€
    ì œëª©/ë‚ ì§œë¥¼ íŒŒì‹±í•œë‹¤.
    ë°˜í™˜: [{id, title, url, date}]
    """
    import re
    from bs4 import BeautifulSoup
    from urllib.parse import urljoin

    # 1) ëª©ë¡ HTML ë°›ê¸° (íŒŒë¼ë¯¸í„° í¬í•¨ ë²„ì „ ì‚¬ìš©)
    resp = http_get(LIST_URL_WITH_PARAMS, headers={
        "Referer": LIST_URL,
        "Accept-Language": "ko,en;q=0.8",
    })
    resp.raise_for_status()
    html = resp.text

    # 2) ëª©ë¡ HTMLì—ì„œ articleNo ìˆ˜ì§‘ (href, onclick ëª¨ë‘ ì»¤ë²„)
    artnos = set(re.findall(r"articleNo=(\d+)", html))
    artnos.update(re.findall(r"fnView\(['\"]?(\d{5,})['\"]?\)", html))
    found = sorted(artnos, reverse=True)[:10]  # ìµœì‹  ëª‡ ê±´ë§Œ
    print(f"[DEBUG] articleNo candidates: {found}")

    items = []
    date_pat = re.compile(r"\d{4}[.\-/]\d{1,2}[.\-/]\d{1,2}")

    # ğŸ”¸ ìƒì„¸ ì œëª© ì¶”ì¶œê¸° (for ë£¨í”„ ì•ì— ì •ì˜)
    def extract_title_from_detail(dsoup):
        BAD_TITLES = {"Calendar", "í†µí•©ê³µì§€", "ëŒ€í•™ìƒí™œ", "ê³µì§€", "ê²Œì‹œíŒ"}

        # 1) og:title
        og = dsoup.select_one('meta[property="og:title"]')
        if og and og.get("content"):
            t = og["content"].strip()
            if t and t not in BAD_TITLES:
                return clean_title(t)

        # 2) "ì œëª©" ë¼ë²¨ ì˜† ê°’(th/td, dt/dd)
        for row in dsoup.select("table tr"):
            th = row.find("th")
            if th and "ì œëª©" in th.get_text(strip=True):
                td = row.find("td")
                if td:
                    t = td.get_text(" ", strip=True)
                    if t and t not in BAD_TITLES:
                        return clean_title(t)
        for dt in dsoup.select("dt"):
            if "ì œëª©" in dt.get_text(strip=True):
                dd = dt.find_next("dd")
                if dd:
                    t = dd.get_text(" ", strip=True)
                    if t and t not in BAD_TITLES:
                        return clean_title(t)

        # 3) ë³¸ë¬¸ ìƒë‹¨ íƒ€ì´í‹€ í›„ë³´
        CANDS = [
            ".board_view .title", ".boardView .title", ".view_title", ".view-title",
            ".bbs_view .tit", ".bbs-view .tit", ".post-title", ".board .title",
            "article h1", "article h2", "#content h1", "#content h2"
        ]
        for sel in CANDS:
            el = dsoup.select_one(sel)
            if el:
                t = el.get_text(" ", strip=True)
                if t and t not in BAD_TITLES and len(t) > 1:
                    return clean_title(t)

        # 4) <title> fallback
        page_title = dsoup.title.get_text(" ", strip=True) if dsoup.title else ""
        if page_title:
            parts = re.split(r"[|\-Â·â€¢Â»Â«]+", page_title)
            parts = [p.strip() for p in parts if p.strip()]
            if parts:
                parts.sort(key=len, reverse=True)
                t = parts[0]
                if t and t not in BAD_TITLES:
                    return clean_title(t)
        return ""

    # 3) ê° ìƒì„¸ í˜ì´ì§€ì—ì„œ ì œëª©/ë‚ ì§œ íŒŒì‹±
    for no in found:
        view_url = f"{LIST_URL}?mode=view&articleNo={no}"
        try:
            d = http_get(view_url, headers={
                "Referer": LIST_URL_WITH_PARAMS,
                "Accept-Language": "ko,en;q=0.8",
            })
            d.raise_for_status()
        except Exception as e:
            print(f"[DEBUG] detail fetch failed for {no}: {e}")
            continue

        dsoup = BeautifulSoup(d.text, "html.parser")
        title_text = extract_title_from_detail(dsoup)

        # ë‚ ì§œ: ë©”íƒ€ ì˜ì—­ ìš°ì„ , ì—†ìœ¼ë©´ ë³¸ë¬¸ì—ì„œ íŒ¨í„´
        date_text = ""
        meta = dsoup.select_one(".date, .regdate, time, .write, .info")
        if meta:
            date_text = meta.get_text(" ", strip=True)
            m = date_pat.search(date_text)
            if m:
                date_text = m.group(0)
        if not date_text:
            m = date_pat.search(dsoup.get_text(" ", strip=True))
            if m:
                date_text = m.group(0)

        if not title_text:
            title_text = f"articleNo {no}"

        items.append({
            "id": f"articleNo:{no}",
            "title": title_text,
            "url": view_url,
            "date": date_text
        })

    # ì¤‘ë³µ ì œê±°
    dedup = {}
    for it in items:
        dedup[it["id"]] = it
    return list(dedup.values())

def send_discord(item):
    # 1ï¸âƒ£ ì œëª© ì •ë¦¬
    base_title = clean_title(item["title"])

    # 2ï¸âƒ£ ë¶€ì„œëª…([í•™ìƒë³µì§€íŒ€] ë“±)ì„ ì°¾ì•„ êµµê²Œ(**â€¦**) í‘œì‹œ
    dept_match = re.match(r'^\[(.*?)\]', base_title)
    if dept_match:
        dept_name = dept_match.group(1)
        base_title = base_title.replace(f"[{dept_name}]", f"**[{dept_name}]**", 1)

    # 3ï¸âƒ£ ì œëª©ì´ ë„ˆë¬´ ê¸¸ë©´ ì˜ë¼ì£¼ê¸° (ê°€ë…ì„±)
    MAX_TITLE = 140
    if len(base_title) > MAX_TITLE:
        base_title = base_title[:MAX_TITLE - 1] + "â€¦"

    # 4ï¸âƒ£ ì¹´ë“œí˜• ë©”ì‹œì§€ í¬ë§·
    content = (
        f"ğŸ“¢ {base_title}\n"
        f"ğŸ“… **ê²Œì‹œì¼:** {item['date'] or 'ë¯¸í‘œê¸°'}\n"
        f"ğŸ”— <{item['url']}>"
    )

    # 5ï¸âƒ£ ë””ìŠ¤ì½”ë“œ ì „ì†¡ (ë ˆì´íŠ¸ ë¦¬ë°‹ ëŒ€ì‘)
    r = requests.post(WEBHOOK_URL, json={"content": content}, timeout=TIMEOUT)
    if r.status_code == 429:
        retry_after = r.json().get("retry_after", 2)
        time.sleep(retry_after)
        requests.post(WEBHOOK_URL, json={"content": content}, timeout=TIMEOUT)
    else:
        r.raise_for_status()

def main():
    seen = load_seen()

    # [1] ì§€ê¸ˆ íŒŒì‹± ì¤‘ì¸ URL í‘œì‹œ
    print(f"[DEBUG] Parsing list from: {LIST_URL}")

    # [2] ì‹¤ì œ HTML íŒŒì‹± í›„ ëª‡ ê°œ í•­ëª©ì„ ì°¾ì•˜ëŠ”ì§€ í‘œì‹œ
    items = fetch_list_items()
    
    print(f"[DEBUG] Fetched items: {len(items)}")

    items = list(items)

    # [3] 'ìƒˆ ê³µì§€'ë¡œ ë¶„ë¥˜ëœ í•­ëª© ê°œìˆ˜ í‘œì‹œ
    new_items = [it for it in items if it["id"] not in seen and match_keywords(it["title"])]
    print(f"[DEBUG] New items: {len(new_items)} (seen={len(seen)})")

    if not new_items:
        print("ìƒˆ ê³µì§€ ì—†ìŒ")
        return

    # ë„ˆë¬´ ë§ìœ¼ë©´ ìƒí•œ
    to_send = new_items[:MAX_SEND_PER_RUN]

    for it in reversed(to_send):  # ì˜¤ë˜ëœ ìˆœìœ¼ë¡œ ì°¨ë¶„íˆ ì „ì†¡
        try:
            send_discord(it)
            seen.add(it["id"])
            time.sleep(0.6)  # ê³¼ë„í•œ ì—°ì† í˜¸ì¶œ ë°©ì§€
        except Exception as e:
            print("ì „ì†¡ ì‹¤íŒ¨:", e)

    save_seen(seen)
    print(f"ì „ì†¡ ì™„ë£Œ: {len(to_send)}ê±´")

if __name__ == "__main__":
    main()
