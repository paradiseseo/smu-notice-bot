# main.py
import os, re, json, time, hashlib
from urllib.parse import urljoin, urlparse, parse_qs
import requests
from bs4 import BeautifulSoup

BASE = "https://www.smu.ac.kr"
LIST_URL = "https://www.smu.ac.kr/kor/life/notice.do"
USER_AGENT = "Mozilla/5.0 (compatible; smu-notice-bot/1.0; +https://www.smu.ac.kr)"
TIMEOUT = 20

STATE_PATH = "state.json"
MAX_SEND_PER_RUN = 10      # 1íšŒ ì‹¤í–‰ ì‹œ ìµœëŒ€ ì „ì†¡ ê°œìˆ˜ (ìŠ¤íŒ¸ ë°©ì§€)
KEYWORDS = [               # (ì„ íƒ) í•„í„°ë§ í‚¤ì›Œë“œ â€” ì´ˆê¸°ì—ëŠ” ëª¨ë‘ ì „ì†¡í•˜ë ¤ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ë‘ì„¸ìš”.
    # r"ì¥í•™", r"ë“±ë¡", r"ìˆ˜ê°•", r"ì±„ìš©", r"ëª¨ì§‘", r"ê³µëª¨ì „", r"ëŒ€íšŒ", r"í–‰ì‚¬"
]

WEBHOOK_URL = os.environ.get("DISCORD_WEBHOOK_URL")
if not WEBHOOK_URL:
    raise SystemExit("í™˜ê²½ë³€ìˆ˜ DISCORD_WEBHOOK_URL ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

def send_test():
    import requests
    requests.post(WEBHOOK_URL, json={"content":"ğŸ§ª FORCE_SEND í…ŒìŠ¤íŠ¸"}, timeout=TIMEOUT)

if os.environ.get("FORCE_SEND") == "1":
    print("[DEBUG] FORCE_SEND=1 â†’ í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€ ì „ì†¡")
    send_test()

def load_seen():
    try:
        with open(STATE_PATH, "r", encoding="utf-8") as f:
            data = json.load(f)
            if isinstance(data, list):
                return set(data)
            return set(data.get("seen", []))
    except FileNotFoundError:
        return set()

def save_seen(seen_set):
    with open(STATE_PATH, "w", encoding="utf-8") as f:
        json.dump(sorted(seen_set), f, ensure_ascii=False, indent=2)

def http_get(url, **kwargs):
    headers = kwargs.pop("headers", {})
    headers["User-Agent"] = USER_AGENT
    return requests.get(url, headers=headers, timeout=TIMEOUT, **kwargs)

def extract_id_from_url(href: str) -> str:
    """
    URL ì¿¼ë¦¬ì˜ no, bbsNo ê°™ì€ íŒŒë¼ë¯¸í„°ê°€ ìˆìœ¼ë©´ ê·¸ê±¸ ì“°ê³ ,
    ì—†ìœ¼ë©´ hrefë¥¼ í•´ì‹œë¡œ IDí™”.
    """
    try:
        qs = parse_qs(urlparse(href).query)
        for k in ("no", "bbsNo", "articleNo", "nttNo"):
            if k in qs and qs[k]:
                return f"{k}:{qs[k][0]}"
    except Exception:
        pass
    return "hash:" + hashlib.sha1(href.encode("utf-8")).hexdigest()[:16]

def guess_category(title: str) -> str:
    cats = [
        ("[ì¥í•™]", r"ì¥í•™|scholar"),
        ("[í•™ì‚¬]", r"ìˆ˜ê°•|íœ´í•™|ë³µí•™|ë“±ë¡|í•™ì |ì„±ì |ì¡¸ì—…|í•™ì‚¬|ìˆ˜ì—…"),
        ("[ì±„ìš©]", r"ì±„ìš©|ì¸í„´|ëª¨ì§‘"),
        ("[í–‰ì‚¬]", r"í–‰ì‚¬|ì„¤ëª…íšŒ|ì„¸ë¯¸ë‚˜|íŠ¹ê°•|ë°•ëŒíšŒ"),
        ("[ê³µëª¨ì „]", r"ê³µëª¨ì „|ëŒ€íšŒ|ì½˜í…ŒìŠ¤íŠ¸|ì±Œë¦°ì§€"),
        ("[ê³µì§€]", r".*"),  # fallback
    ]
    for tag, pattern in cats:
        if re.search(pattern, title, flags=re.I):
            return tag
    return "[ê³µì§€]"

def match_keywords(title: str) -> bool:
    if not KEYWORDS:
        return True
    return any(re.search(p, title, flags=re.I) for p in KEYWORDS)

def fetch_list_items():
    """
    ìƒëª…ëŒ€ í†µí•©ê³µì§€ ì „ìš© íŒŒì„œ(ê°•í™”íŒ)
    - ì œëª© a íƒœê·¸ì˜ href ë˜ëŠ” onclickì—ì„œ articleNoë¥¼ ì¶”ì¶œ
    - href=...mode=view&articleNo=... ì´ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    - href="#" ì´ê³  onclick="fnView('760387')" ë“±ì¸ ê²½ìš° ìˆ«ì ì¶”ì¶œ í›„ URL êµ¬ì„±
    - ë‚ ì§œëŠ” ê°™ì€ tr/ì¸ì ‘ ë¸”ë¡ì—ì„œ yyyy.mm.dd, yyyy-mm-dd, yyyy/mm/dd íŒ¨í„´ íƒìƒ‰
    ë°˜í™˜: [{id, title, url, date}]
    """
    import re
    from bs4 import BeautifulSoup
    from urllib.parse import urljoin, urlparse, parse_qs

    resp = http_get(LIST_URL)
    resp.raise_for_status()
    html = resp.text
    soup = BeautifulSoup(html, "html.parser")

    items = []
    date_pat = re.compile(r"\d{4}[.\-/]\d{1,2}[.\-/]\d{1,2}")

    def extract_article_no_from_href(href: str):
        if not href:
            return None
        qs = parse_qs(urlparse(urljoin(BASE, href)).query)
        if "articleNo" in qs and qs["articleNo"]:
            return qs["articleNo"][0]
        return None

    def extract_article_no_from_onclick(attr: str):
        if not attr:
            return None
        # fnView('760387') / goView(760387) / view( '760387' ) ë“± ìˆ«ì 6ìë¦¬ ì´ìƒ
        m = re.search(r"(?<!\d)(\d{5,})(?!\d)", attr)
        return m.group(1) if m else None

    def build_view_url(article_no: str):
        # ëª©ë¡ì—ì„œ ì“°ë˜ ê¸°ë³¸ íŒŒë¼ë¯¸í„°ëŠ” ì—†ì–´ë„ ìƒì„¸ëŠ” ì—´ë¦½ë‹ˆë‹¤. ìµœì†Œ êµ¬ì„±ìœ¼ë¡œ ìƒì„±
        return f"{LIST_URL}?mode=view&articleNo={article_no}"

    # ì œëª©ì´ ë“¤ì–´ìˆëŠ” ì•µì»¤ í›„ë³´ ë„“ê²Œ ìˆ˜ì§‘ (ê²Œì‹œíŒ ì˜ì—­ ìš°ì„ )
    # í…Œì´ë¸”/ë¦¬ìŠ¤íŠ¸ ëª¨ë‘ ì»¤ë²„: ì œëª© í…ìŠ¤íŠ¸ê°€ 2ê¸€ì ì´ìƒì¸ a íƒœê·¸
    anchors = soup.select("table a[href], table a[onclick], .board-list a, a")
    for a in anchors:
        title = a.get_text(strip=True)
        if not title or len(title) < 2:
            continue

        href = a.get("href") or ""
        onclick = a.get("onclick") or ""

        # 1) hrefì—ì„œ articleNo ì¶”ì¶œ
        art_no = extract_article_no_from_href(href)

        # 2) hrefê°€ ì—†ê±°ë‚˜ '#'ì´ê³ , onclickì— ìˆ«ìê°€ ìˆìœ¼ë©´ ì¶”ì¶œ
        if not art_no and (href in ("", "#", "javascript:void(0)", "javascript:;") or "mode=view" not in href):
            art_no = extract_article_no_from_onclick(onclick)

        # articleNoê°€ ì—†ìœ¼ë©´ ê²Œì‹œê¸€ë¡œ ë³´ì§€ ì•ŠìŒ
        if not art_no:
            continue

        full = build_view_url(art_no)

        # ë‚ ì§œ ì¶”ì¶œ: ê°™ì€ tr ìš°ì„ , ì—†ìœ¼ë©´ ë¶€ëª¨ ë¸”ë¡ë“¤ì—ì„œ ê²€ìƒ‰
        date_text = ""
        tr = a.find_parent("tr")
        if tr:
            for td in reversed(tr.find_all("td")):
                txt = td.get_text(" ", strip=True)
                m = date_pat.search(txt)
                if m:
                    date_text = m.group(0)
                    break
        if not date_text:
            parent = a.find_parent(["li", "div", "article", "section"]) or a.parent
            if parent:
                cand = parent.select_one(".date, .regdate, time")
                if cand:
                    date_text = cand.get_text(strip=True)
                else:
                    m = date_pat.search(parent.get_text(" ", strip=True))
                    if m:
                        date_text = m.group(0)

        items.append({
            "id": f"articleNo:{art_no}",
            "title": title,
            "url": full,
            "date": date_text
        })

    # ë©”ë‰´/ì¤‘ë³µ ì œê±°: articleNo ê¸°ì¤€ìœ¼ë¡œ dedup
    dedup = {}
    for it in items:
        # ëª©ë¡ ë£¨íŠ¸(ìê¸° ìì‹ )ë‚˜ ë©”ë‰´ í…ìŠ¤íŠ¸(ëŒ€í•™ìƒí™œ/í†µí•©ê³µì§€) í•„í„°
        if it["url"].split("?")[0].rstrip("/") == LIST_URL.rstrip("/"):
            continue
        if it["title"] in ("ëŒ€í•™ìƒí™œ", "í†µí•©ê³µì§€"):
            continue
        dedup[it["id"]] = it

    # ì›ë˜ í™”ë©´ ìˆœì„œ ê·¼ì‚¬ ìœ ì§€
    ordered = list(dedup.values())
    return ordered

def send_discord(item):
    tag = guess_category(item["title"])
    content = (
        f"ğŸ“¢ **ìƒˆ ê³µì§€** {tag}\n"
        f"**ì œëª©**: {item['title']}\n"
        f"**ê²Œì‹œì¼**: {item['date'] or 'ë¯¸í‘œê¸°'}\n"
        f"ğŸ”— {item['url']}"
    )
    # ê°„ë‹¨í•œ rate-limit ëŒ€ì‘ (WebhookëŠ” ë³´í†µ 5req/2s)
    r = requests.post(WEBHOOK_URL, json={"content": content}, timeout=TIMEOUT)
    if r.status_code == 429:
        retry_after = r.json().get("retry_after", 2)
        time.sleep(retry_after)
        requests.post(WEBHOOK_URL, json={"content": content}, timeout=TIMEOUT)
    else:
        r.raise_for_status()

print(f"[DEBUG] Parsing list from: {LIST_URL}")
def main():
    seen = load_seen()

    # [1] ì§€ê¸ˆ íŒŒì‹± ì¤‘ì¸ URL í‘œì‹œ
    print(f"[DEBUG] Parsing list from: {LIST_URL}")

    # [2] ì‹¤ì œ HTML íŒŒì‹± í›„ ëª‡ ê°œ í•­ëª©ì„ ì°¾ì•˜ëŠ”ì§€ í‘œì‹œ
    items = fetch_list_items()
    
    print(f"[DEBUG] Fetched items: {len(items)}")

    items = list(items)

    # [3] 'ìƒˆ ê³µì§€'ë¡œ ë¶„ë¥˜ëœ í•­ëª© ê°œìˆ˜ í‘œì‹œ
    new_items = [it for it in items if it["id"] not in seen and match_keywords(it["title"])]
    print(f"[DEBUG] New items: {len(new_items)} (seen={len(seen)})")

    if not new_items:
        print("ìƒˆ ê³µì§€ ì—†ìŒ")
        return

    # ë„ˆë¬´ ë§ìœ¼ë©´ ìƒí•œ
    to_send = new_items[:MAX_SEND_PER_RUN]

    for it in reversed(to_send):  # ì˜¤ë˜ëœ ìˆœìœ¼ë¡œ ì°¨ë¶„íˆ ì „ì†¡
        try:
            send_discord(it)
            seen.add(it["id"])
            time.sleep(0.6)  # ê³¼ë„í•œ ì—°ì† í˜¸ì¶œ ë°©ì§€
        except Exception as e:
            print("ì „ì†¡ ì‹¤íŒ¨:", e)

    save_seen(seen)
    print(f"ì „ì†¡ ì™„ë£Œ: {len(to_send)}ê±´")

if __name__ == "__main__":
    main()
